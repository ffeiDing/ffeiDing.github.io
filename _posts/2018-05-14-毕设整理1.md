---
layout: post
published : false
---

似乎不能完美解决数据集可能有部分重复的问题。
可能是get_prob_id_list并不能获得所有prob_id

换句话说，假设给定论文是
zhang san@li si@

那么当zhang san匹配对数大于等于2时就自动跳出，返回prob_id_list
不会把所有的作者可能匹配的id都考虑到。

实际上这也是合理的，毕竟是人名消歧，如果zhang san匹配的名字的论文 和 li si匹配的名字的论文放一块，这时候就感觉有点怪，毕竟这两者的名字是不重名的。


还有一个可能，useful paper是在guess_total中遍历根据匹配对数判断一定需要消歧的论文。 只统计了useful paper  
而ambiguous set是按prob_id_list里的id对应的论文抽取的，可能会多抽一点。因为可以抽useless的paper
但是这个可能性似乎也很小，因为一个id的某一篇论文需要消歧，那么这个id的其他论文也很大可能需要消歧，因为名字没大的变化。   
但也有可能这个id署名的时候，格式不一样（甚至zhou jie lun写成jie lun zhou，因为guess_total寻找id是按标题找的），并且还有可能有多音字。

还有一个可能，ambiguous_set是通过遍历标记样本去找可能的id对应的论文，并放入一个txt，但没有把当前所遍历的样本放入txt。见论文。




生成的ambiguous set38 只有一个worker id，没有第二类。而送入的prob_id_list却有两个。
>>> import get_ambiguous_sets
>>> print(get_prob_id_list('lu lu@nan jie@mi wei@li lan fen@wei chun hong@su xiao dong@li yi'))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'get_prob_id_list' is not defined
>>> print(get_ambiguous_sets.get_prob_id_list('lu lu@nan jie@mi wei@li lan fen@wei chun hong@su xiao dong@li yi'))
['0006161207', '0006166482']

这两个id查了下都是有需要消歧的论文的。
可以推测是下面这行的逻辑是错的
if this_line_title not in global_paper_title:

因为之前已经加入了0006166482的论文，因此，这次不被加入，导致最后ambiguous set中只有一个类。

from get_ambiguous_sets.py

```python
def get_prob_id_list(authors):
    #authors为某篇论文的作者和合作者
    author_list = authors.strip().split('@')
    save_prob_id_list = []
    for author in author_list:
        if len(author) < 2: #如果作者名字的字符小于2，则跳过。name_pinyin_id.txt中的len(name_pinyin)必然是大于等于2的
            continue
        #如果在这里声明prob_id_list
        #遍历论文作者时会清空
        prob_id_list = []
        #print(author)
        with codecs.open('./rawdata/valid_names_pinyin_and_id.txt', 'r', 'utf-8') as f:
            for line in f.readlines():
               L = line.strip().split('\001')
               if len(L) > 1:
                   #L[0]为pinyin的string
                   #需要考虑空格的问题
                   author = author.replace(' ','')
                   if L[0].replace(' ','').find(author.strip() + '@') != -1:
                       prob_id_list.append(L[1].strip())

        if len(prob_id_list) > 1:
            break
        if len(prob_id_list) == 1:
            save_prob_id_list = prob_id_list

        #logging.debug('prob_id_list : {}'.format(prob_id_list))
    if len(save_prob_id_list) > len(prob_id_list):
        prob_id_list = save_prob_id_list

    return prob_id_list
```

from main.py
```python
def get_prob_id_list(authors):
    #authors为某篇论文的作者和合作者
    author_list = authors.strip().split('@')
    '''
    这里有点问题，prob_id_list是在for里面声明的。
    每次循环会清空。
    '''
    save_prob_id_list = []
    for author in author_list:
        if len(author) < 2: #如果作者名字的字符小于2，则跳过。name_pinyin_id.txt中的len(name_pinyin)必然是大于等于2的
            continue
        prob_id_list = []

        with codecs.open('./rawdata/valid_names_pinyin_and_id.txt', 'r', 'utf-8') as f:
            for line in f.readlines():
               L = line.strip().split('\001')
               if len(L) > 1:
                   #L[0]为pinyin的string
                   if L[0].find(author.strip() + '@') != -1:
                       prob_id_list.append(L[1].strip())
        if len(prob_id_list) > 1 :
            break
        if len(prob_id_list) == 1:
            save_prob_id_list = prob_id_list

    #不管len(prob_id_list)如何，都要重新写文件，以保证清空，这样才不会导致上一次训练的数据混到了下一次
    if len(save_prob_id_list) > len(prob_id_list):
        prob_id_list = save_prob_id_list
    if len(prob_id_list) == 0:
        with codecs.open('./train_data/given_paper_prob_ids.txt', 'w', 'utf-8') as f:
            f.write(str(prob_id_list) + '\n')
        with codecs.open('./train_data/all_paper_prob_ids.txt', 'a', 'utf-8') as f:
            f.write(str(prob_id_list) + '\n')
    elif len(prob_id_list) >= 1:
        with codecs.open('./train_data/given_paper_prob_ids.txt', 'w', 'utf-8') as f:
            f.write('@'.join(prob_id_list) + '\n')
        with codecs.open('./train_data/all_paper_prob_ids.txt', 'a', 'utf-8') as f:
            f.write('@'.join(prob_id_list) + '\n')
    return len(prob_id_list)
```


`k@localhost  ~/Documents/codes/paper_author_process   comments ●  python make_stats.py
`

```

useful_paper_num :  1439
useless_paper_num :  6351
ambiguous_sets_paper_num : 1580
ambiguous_sets_paper_num2 : 1580
INFO:root:the total number of duplicate_name_cns is : 21
INFO:root:duplicate_name_cn2person_num:
INFO:root:duplicate_name_cn : 朱瑞, person_num : 2
INFO:root:duplicate_name_cn : 杨爽, person_num : 2
INFO:root:duplicate_name_cn : 马宏骥, person_num : 2
INFO:root:duplicate_name_cn : 杜晓峰, person_num : 2
INFO:root:duplicate_name_cn : 刘锋, person_num : 3
INFO:root:duplicate_name_cn : 张洁, person_num : 2
INFO:root:duplicate_name_cn : 林坚, person_num : 2
INFO:root:duplicate_name_cn : 董巍, person_num : 2
INFO:root:duplicate_name_cn : 李晟, person_num : 2
INFO:root:duplicate_name_cn : 王昊, person_num : 2
INFO:root:duplicate_name_cn : 洪龙, person_num : 2
INFO:root:duplicate_name_cn : 王莉, person_num : 2
INFO:root:duplicate_name_cn : 陈萍, person_num : 2
INFO:root:duplicate_name_cn : 张颖, person_num : 2
INFO:root:duplicate_name_cn : 杨莹, person_num : 2
INFO:root:duplicate_name_cn : 李宁, person_num : 2
INFO:root:duplicate_name_cn : 王凯, person_num : 2
INFO:root:duplicate_name_cn : 郭强, person_num : 2
INFO:root:duplicate_name_cn : 王玮, person_num : 2
INFO:root:duplicate_name_cn : 张岩, person_num : 2
INFO:root:duplicate_name_cn : 陈峰, person_num : 2
has_paper_ids_num : 762

```


make_stats.py

```python

# -*- coding: utf-8 -*-
import codecs
import pypinyin
import get_name_pinyin
import logging
import mark_cleaned_crawl_data
import os
logging.basicConfig(level=logging.INFO)
id2papers_num = {}

def get_id2papers_num():
    with codecs.open('./rawdata/guess_total.txt', 'r', 'utf-8') as f:
        for line in f.readlines():
            L = line.strip().split('\001')
            if len(L) > 6:
                person_info = L[6].strip().split('#')
                if len(person_info) > 2:
                    worker_id = person_info[2]
                    if worker_id.strip() in id2papers_num.keys():
                        id2papers_num[worker_id] += 1
                    else:
                        id2papers_num[worker_id] = 1


def get_ambiguous_papers_num():
    fp = codecs.open('./rawdata/guess_total.txt', 'r', 'utf-8')
    fp1 = codecs.open('./rawdata/name_pinyin_id.txt', 'r', 'utf-8')
    count = 0
    for line in fp.readlines():
        L = line.strip().split('\001')
        if len(L) > 6:
            person_info = L[6].strip().split('#')
            pinyinlist = pypinyin.pinyin(L[5].strip(), style=pypinyin.NORMAL, heteronym=True)
            get_name_pinyin.cur_names_in_pinyin = []
            get_name_pinyin.dfs(pinyinlist, 0, '', '')
            prob_pinyin_names = get_name_pinyin.get_each_prob_name()

            is_ambiguous = False
            for name in prob_pinyin_names:
                with codecs.open('./rawdata/name_pinyin_id.txt', 'r', 'utf-8') as f2:
                    for line2 in f2.readlines():
                        if line2.find(name.strip() + '@') != -1 and line2.find(person_info[2].strip()) == -1:
                            is_ambiguous = True#同一个pinyin_name,但职工号不同
                            break
                if is_ambiguous:
                    break
            if is_ambiguous == True:
                logging.debug('the chinese name of the author : %s'.format(L[4].strip()))
                logging.debug('prob_pinyin_names : %s'.format(prob_pinyin_names))
                logging.debug('one prob_pinyin_name : %s'.format(name))
                count += 1
    return count


def get_ambiguous_sets_paper_num():
    ambiguous_sets_paper_num = 0
    rootdir = os.path.abspath('./rawdata/ambiguous_sets/')
    for filename in os.listdir(rootdir):
        if filename[-9:] != '.DS_Store':
            filepath = os.path.join(rootdir,filename)
            with codecs.open(filepath,'r','utf-8') as f:
                corpus = f.readlines()
            ambiguous_sets_paper_num += len(corpus)
    return ambiguous_sets_paper_num


def get_ambiguous_sets_paper_num2():
    ambiguous_sets_paper_num2 = 0
    rootdir = os.path.abspath('./rawdata/ambiguous_sets/')
    paper_title_set = set()
    for filename in os.listdir(rootdir):
        if filename[-9:] != '.DS_Store':
            filepath = os.path.join(rootdir,filename)
            with codecs.open(filepath,'r','utf-8') as f:
                for line in f.readlines():
                    L = line.strip().split("\001")
                    if L[0].strip() not in paper_title_set:
                        ambiguous_sets_paper_num2 += 1
                        paper_title_set.add(L[0].strip())
    return ambiguous_sets_paper_num2


def get_useful_paper_num():
    useful_paper_num,useless_paper_num = 0,0
    rootdir = os.path.abspath('./rawdata/paper_class/useful_paper/')
    for filename in os.listdir(rootdir):
        if filename[-9:] != '.DS_Store':
            filepath = os.path.join(rootdir,filename)
            with codecs.open(filepath,'r','utf-8') as f:
                corpus = f.readlines()
            useful_paper_num += len(corpus)

    rootdir = os.path.abspath('./rawdata/paper_class/useless_paper/')
    for filename in os.listdir(rootdir):
        if filename[-9:] != '.DS_Store':
            filepath = os.path.join(rootdir,filename)
            with codecs.open(filepath,'r','utf-8') as f:
                corpus = f.readlines()
            useless_paper_num += len(corpus)

    print('useful_paper_num : ',useful_paper_num)
    print('useless_paper_num : ',useless_paper_num)


if __name__ == '__main__':
    get_useful_paper_num()
    print('ambiguous_sets_paper_num :',get_ambiguous_sets_paper_num())
    print('ambiguous_sets_paper_num2 :',get_ambiguous_sets_paper_num2())

    mark_cleaned_crawl_data.name_cn2person_infos = {}
    mark_cleaned_crawl_data.get_name_cn2person_infos()
    duplicate_name_cn2person_num = {}
    for name_cn,person_infos in mark_cleaned_crawl_data.name_cn2person_infos.items():
        if len(person_infos) > 1:
            duplicate_name_cn2person_num[name_cn] = len(person_infos)
    logging.info('the total number of duplicate_name_cns is : {}'.format(len(duplicate_name_cn2person_num)))
    logging.info('duplicate_name_cn2person_num:')
    for k in duplicate_name_cn2person_num.keys():
        logging.info('duplicate_name_cn : {}, person_num : {}'.format(k,duplicate_name_cn2person_num[k]))

    get_id2papers_num()
    #logging.info('id2papers_num map : {}'.format(sorted(id2papers_num.items(), key = lambda id_papers_num_pair:id_papers_num_pair[1], reverse= True)))
    print('has_paper_ids_num :',len(id2papers_num))

```

get_ambiguous_sets.py  


```python

import codecs
import os
import random
from text_process_tools import *
import train_and_pred
import shutil
import time
import numpy as np
import get_name_pinyin
from sklearn.decomposition import PCA
from sklearn import metrics
import logging
logging.basicConfig(level=logging.INFO)

global_paper_title = set()

def get_prob_id_list(authors):
    #authors为某篇论文的作者和合作者
    author_list = authors.strip().split('@')
    save_prob_id_list = []
    for author in author_list:
        if len(author) < 2: #如果作者名字的字符小于2，则跳过。name_pinyin_id.txt中的len(name_pinyin)必然是大于等于2的
            continue
        #如果在这里声明prob_id_list
        #遍历论文作者时会清空
        prob_id_list = []
        #print(author)
        with codecs.open('./rawdata/valid_names_pinyin_and_id.txt', 'r', 'utf-8') as f:
            for line in f.readlines():
               L = line.strip().split('\001')
               if len(L) > 1:
                   #L[0]为pinyin的string
                   #需要考虑空格的问题
                   author = author.replace(' ','')
                   if L[0].replace(' ','').find(author.strip() + '@') != -1:
                       prob_id_list.append(L[1].strip())

        if len(prob_id_list) > 1:
            break
        if len(prob_id_list) == 1:
            save_prob_id_list = prob_id_list

        #logging.debug('prob_id_list : {}'.format(prob_id_list))
    if len(save_prob_id_list) > len(prob_id_list):
        prob_id_list = save_prob_id_list

    return prob_id_list

def get_ambiguous_set(this_paper_title,prob_id_list,idx):

    global global_paper_title
    paper_title_set = set()
    paper_title_set.add(this_paper_title)
    iswrited = False
    for prob_id in prob_id_list:
        if os.path.exists('./rawdata/paper_class/useful_paper/' + prob_id):
            fread = codecs.open('./rawdata/paper_class/useful_paper/' + prob_id, 'r', 'utf-8')
        elif os.path.exists('./rawdata/paper_class/useless_paper/' + prob_id):
            fread = codecs.open('./rawdata/paper_class/useless_paper/' + prob_id, 'r', 'utf-8')

        #如果数据集中没有该id的论文，那么跳过该作者
        #实际上在valid_names_pinyin_and_id.txt中挑选的id都是有论文的，不会出现找不到论文

        for line in fread.readlines():
            L = line.split('\001')
            if len(L) > 6:
                this_line_title = L[0]
                #不确定这里的逻辑是否正确
                if this_line_title not in global_paper_title:
                    global_paper_title.add(this_line_title)
                    if not (this_line_title in paper_title_set):
                        paper_title_set.add(this_line_title)
                        with codecs.open('./rawdata/ambiguous_sets/ambiguous_set' + str(idx), 'a', 'utf-8') as f:
                            f.write(line.strip() + '\n')
                            if not iswrited:
                                iswrited = True

    if iswrited:
        print("ambiguous_set"+str(idx)+' is completed!')
        return idx+1
    else:
        return idx


def get_ambiguous_sets():
    if not os.path.exists('./rawdata/ambiguous_sets/'):
        os.mkdir('./rawdata/ambiguous_sets/')
    fread = codecs.open('./rawdata/guess_total.txt', 'r', 'utf-8')
    idx = 1
    global global_paper_title
    global_paper_title.clear()
    get_name_pinyin.id2name_pinyin = {}
    get_name_pinyin.id2name_cn = {}
    get_name_pinyin.get_id2name_pinyin()
    for line in fread.readlines():
        L = line.split('\001')
        if len(L) > 5:
            this_paper_title = L[0]
            if this_paper_title not in global_paper_title:
                global_paper_title.add(this_paper_title)
            else:
                continue
            authors = L[1] #作者及合作者
            prob_id_list = get_prob_id_list(authors.strip())
            prob_id_nums = len(prob_id_list)
            #excel中职工的中文的大部分拼音都在name_pinyin_id.txt中
            #如果isempty_prob_id_list == True 说明该论文不是excel里职工写的 和excel里的职工毫无关联（除非拼音还有遗漏）
            #因此无法预测
            #这里只考虑了pinyin情况，但是有对应的pinyin不一定有论文
            if prob_id_nums == 0:
                continue
            #无须预测
            if prob_id_nums == 1:
                continue
            idx = get_ambiguous_set(this_paper_title,prob_id_list,idx)


    fread.close()
if __name__ == '__main__':
    get_ambiguous_sets()

```
