---
layout: post
tags: [sklearn,nlp,ml]
published : true
---

sublinear_tf : boolean, default=False   

Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).    

http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html    
```
"-------输出第 0 类文本的词语tf-idf权重------\n",
     "中国 0.0\n",
     "北京 0.5264054336099155\n",
     "大厦 0.0\n",
     "天安门 0.0\n",
     "小明 0.0\n",
     "来到 0.5264054336099155\n",
     "杭研 0.0\n",
     "毕业 0.0\n",
     "清华大学 0.6676785446095399\n",
     "硕士 0.0\n",
     "科学院 0.0\n",
     "网易 0.0\n",
     "-------输出第 1 类文本的词语tf-idf权重------\n",
     "中国 0.0\n",
     "北京 0.0\n",
     "大厦 0.5254727492640658\n",
     "天安门 0.0\n",
     "小明 0.0\n",
     "来到 0.41428875116588965\n",
     "杭研 0.5254727492640658\n",
     "毕业 0.0\n",
     "清华大学 0.0\n",
     "硕士 0.0\n",
     "科学院 0.0\n",
     "网易 0.5254727492640658\n",
```
tfidf本身基于词袋模型、VSM.
换句话说，只要这个词在语料中出现了，那么就会被当成feature.   
tfidf 很大可能会遇上高维稀疏的问题。
通过设置max_df和min_df会去掉一些词。   
注意同一个词的权重也是变化的。
实际上不能说是权重：每个词对应feature，而该词的tfidf值对应该feature的值
实际上tfidf不应该称为计算权重的方法，而应该被称为计算feature值的方法。   
举例来说，假设 x = [x1,x2,...,xn]           
某一个sample为[0.2,0.3,...,0.6],   
用最简单的线性回归，那么   
pred = w1*0.2 + w2*0.3 + ... + wn*0.6   
我们可以看到tfidf只是用来计算feature的值，至于计算每个feature对应的权重，则由具体的模型（如LR,SVM）来解决.
