---
layout: post
published : true
tags : [ml]
---



[1]这份材料相当深入浅出。

李航的书里把第K类另外处理了，其实是没有必要的，因为第K类和其他类的地位是一样的。[1]中的公式更好理解。

[1]中讲到softmax具有overparameterized，有多解，换句话说两组不同参数的softmax，给出的分布也有可能完全一样。
有点类似线性代数中不同参数的平行向量。

使用牛顿迭代法会遇到问题。

one could instead set
$$\theta_1 = \vec{0}$$

and optimize only with respect to the (k − 1)(n + 1) remaining parameters, and this would work fine.

加上正则化之后就变成唯一解了。

softmax 和 k Binary Classifiers  

前者只能解决一个样本只对应一个类的多分类问题，后者可以解决一个样本具有多个类别的问题。   






[1]http://deeplearning.stanford.edu/wiki/index.php/Softmax_Regression
