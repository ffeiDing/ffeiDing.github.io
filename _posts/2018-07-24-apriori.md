---
layout: post
published : true
tags : [他山之石,数据挖掘]
---

啤酒与尿布
>Apriori: “先验”

[https://github.com/kifish/process-pathology/blob/dev/analysis/apriori.py
](https://github.com/kifish/process-pathology/blob/dev/analysis/apriori.py
)


从output来理解apriori:    
推出的一条规则可能形式如下:         
a,b,c... -> e    confidence:0.65 support:0.06     
可将左边记为集合A,右边记为集合B    
A->B,实际上对应了:P(B|A) 而 confidence = P(B|A) 

举例来说,当顾客购买了啤酒,推荐该顾客买尿布是根据最大似然:   
$$disaper = argmax_{item}P(item|beer)$$
似然概率可如下计算:   
$$P(A|B)=P(A,B)/P(B)=P(A\cap B)/P(B)$$

假设超市中共n件商品,分进两个集合:A集合和B集合,保证两个集合皆非空.  
则有2^n-2种组合方式:         
$$C_n^1 + C_n^2 + ... + C_n^{n-1}= 2^n - 2$$

假设n=10000,则组合方式的个数 > 10^3010    
靠brute-force来计算每种组合的概率是不可行的.

apriori取了巧,将集合B中元素的个数限定为1个,这样即可通过递推来计算$P(A|B)$.极大地减小了计算复杂度.

回到商品推荐的场景,当顾客购买了多件商品再推荐顾客购买其他的多件商品(而不是单件商品) 似乎更自然一些.

apriori不能用来直接推荐多件商品. 但可以"链式"推荐:
假设
```
    a,b->c       confidence 0.7        support 0.05 
    a,b,c->d     confidence 0.6	       support 0.04
    ...
```
当顾客的购物车里有商品a和商品b,这时候按最大似然(也就是置信度最大)给顾客推荐了商品c,顾客把商品c加入了购物车,再次遍历规则集合,继续按最大似然推荐d即可.

显然,经过"链式"推荐,每次推荐的商品的最大似然概率是慢慢减小的.
没必要直接推荐商品{c,d},因为:
$$P({c,d}|{a,b}) <= P({c}|{a,b}) and P({c,d}|{a,b})<= P({d}|{a,b})$$只需根据购物车的商品推测最有可能购买的一个商品即可.

至于这里的概率,是通过频率来近似的.


附:

<code>import numpy as np </code>


```python

import numpy as np
class Apriori:
    def __init__(self,min_support,min_confidence,max_length = 10000000):
        self.min_support = min_support # 最小支持度
        self.min_confidence = min_confidence # 最小置信度
        self.max_length = max_length # 频繁项的最大长度
    
    def count(self,filename = 'data.txt'):
        self.total = 0 # 数据总行数
        item2num = {} #物品集合

        #统计得到物品清单
        with open(filename) as f:
            for line in f:
                self.total += 1
                for item in line.strip().split(','):
                    if item in item2num:
                        item2num[item] += 1
                    else:
                        item2num[item] = 1

        self.item2ratio = {item: num/self.total for item,num in item2num.items() if num/self.total > self.min_support}
        self.item2id = {item: idx for idx,item in enumerate(self.item2ratio)}

        #初始化物品清单的0-1矩阵
        # transaction
        self.D = np.zeros((self.total, len(item2num)),dtype = bool)
        #个人认为初始化成0,1而不是bool可能更好，因为python中的bool不像C语言中的bool那样(几乎和01等价)
        
        #得到物品清单的0-1矩阵
        with open(filename) as f:
            for idx,line in enumerate(f):
                for item in line.strip().split(','):
                    if item in self.item2ratio:
                        self.D[idx,self.item2id[item]] = True 

    def find_rules(self,filename = 'apriori.txt'):
        self.count(filename)
        rules = [{ (item,):ratio for item,ratio in self.item2ratio.items() }]
        cur_item_num = 0 # 当前步的频繁项的物品数

        while rules[-1] and cur_item_num + 2 <= self.max_length:  # 包含了从k频繁项到k+1频繁项的构建过程
            rules.append({}) # 给k+1频繁项 初始化
            freq_itemsets = sorted(rules[-2].keys())  # 对每个k频繁项按字典序排序（核心）感觉未必要字典序,只需要排序即可
            # 这样的排序通过递推,实现了freq_itemset内的item是有序的,且freq_itemsets也是有序的
            pre_freq_itemset_num = len(rules[-2]) # k频繁项的数量
            cur_item_num += 1 # k
            for idx in range(pre_freq_itemset_num):
                for idx2 in range(idx+1, pre_freq_itemset_num):
                    # cur_item_num-1即k-1
                    # 如果前面k-1个item相同，那么这两个k频繁项就可以组合成一个k+1频繁项
                    if freq_itemsets[idx][:cur_item_num-1] == freq_itemsets[idx2][:cur_item_num-1]:
                        # freq_itemsets[idx]:一个tuple
                        # cur_freq_itemset: k+1 频繁项,包含了k+1个不同的item
                        cur_freq_itemset = freq_itemsets[idx] + (freq_itemsets[idx2][cur_item_num-1],)
                        # 拼接成k+1 频繁项
                        # 实际上freq_itemsets[idx]并上freq_itemsets[idx2] 共有k+1个不同的item
                        cur_freq_itemset_id = [self.item2id[item] for item in cur_freq_itemset]
                        # sum(np.prod(self.D[:, cur_freq_itemset_id], 1)) k+1频繁项在数据集中的共现次数
                        support = 1.0 * sum(np.prod(self.D[:, cur_freq_itemset_id],1)) / self.total
                        if support > self.min_support: #判断是否足够频繁 
                            rules[-1][cur_freq_itemset] = support
                        
        # 遍历每一个频繁项，计算置信度
        # 也可以认为是剪枝
        result = {}
        for k, freq_itemset_pair in enumerate(rules[1:]): # 项数大于等于2
            for freq_itemset, support in freq_itemset_pair.items():#对于当前的k,遍历所有k频繁项
                for i, _ in enumerate(freq_itemset):
                    #遍历所有的组化，即(a,b,c)究竟是 a,b -> c 还是 a,c -> b 还是 b,c -> a ？
                    exclude_ith_item = freq_itemset[:i] + freq_itemset[i+1:] # 把第i项剔除
                    confidence = support / rules[k][exclude_ith_item] # 不同组合的置信度(一种排列对于一条规则)
                    if confidence > self.min_confidence:
                        result[exclude_ith_item + (freq_itemset[i],)] = (confidence,support)
                        #例如：
                        #exclude_ith_item + ith_item :a,b,c
                        #exclude_ith_item:a,b
                        #P(a,b,c) = P(a,b)*P(c|a,b)
                        #P(c|a,b) = P(a,b,c)/P(a,b) = support / rules[k][exclude_ith_item]
        

        return sorted(result.items(),key = lambda x:-x[1][0]) # 按置信度confidence降序排列



if __name__ == '__main__':
    from pprint import pprint
    model = Apriori(0.06, 0.75)
    pprint(model.find_rules('data.txt'))
    print('----------')
    model = Apriori(0.06, 0.75,3)
    pprint(model.find_rules('data.txt'))





```



>2000年Jiawei Han等人提出了基于FP树生成频繁项集的FP-growth算法。该算法只进行2次数据库扫描且它不使用侯选集，直接压缩数据库成一个频繁模式树，最后通过这棵树生成关联规则。研究表明它比Apriori算法大约快一个数量级。

我实现过一个将apriori输出的规则放入森林,不知和上述算法是否存在关联.
https://github.com/kifish/process-pathology/blob/dev/analysis/main.py

Reference:    
https://blog.csdn.net/dream_angel_z/article/details/46355803     
https://github.com/FanhuaandLuomu/Apriori_Learning/blob/master/Apriori_numpy.py        
https://spaces.ac.cn/archives/5525        
https://colab.research.google.com/drive/1aXmA3sR5GefpBDM2zdnImpL9X6yQ4hAh
https://en.wikipedia.org/wiki/Apriori_algorithm