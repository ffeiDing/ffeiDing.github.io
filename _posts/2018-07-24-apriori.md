---
layout: post
published : true
tags : [他山之石,数据挖掘,data analysis]
---


>Apriori Algorithm is one of the most important algorithm which is used to extract frequent itemsets from large database and get the association rule for discovering the knowledge. It basically requires two important things: minimum support and minimum confidence. First, we check whether the items are greater than or equal to the minimum support and we find the frequent itemsets respectively. Secondly, the minimum confidence constraint is used to form association rules. 

啤酒与尿布
>Apriori: “先验”

[https://github.com/kifish/process-pathology/blob/dev/analysis/apriori.py
](https://github.com/kifish/process-pathology/blob/dev/analysis/apriori.py
)


从output来理解apriori:    
推出的一条规则可能形式如下:         
a,b,c... -> e    confidence:0.65 support:0.06     
可将左边记为集合A,右边记为集合B    
A->B,实际上对应了:P(B|A) 而 confidence = P(B|A) 

举例来说,当顾客购买了啤酒,推荐该顾客买尿布是根据最大似然:   
$$disaper = argmax_{item}P(item|beer)$$
似然概率可如下计算:   
$$P(A|B)=P(A,B)/P(B)=P(A\cap B)/P(B)$$

假设超市中共n件商品,分进两个集合:A集合和B集合,保证两个集合皆非空.  
则有2^n-2种组合方式:         
$$C_n^1 + C_n^2 + ... + C_n^{n-1}= 2^n - 2$$

假设n=10000,则组合方式的个数 > 10^3010    
靠brute-force来计算每种组合的概率是不可行的.     
apriori取了巧,将集合B中元素的个数限定为1个,这样即可通过递推来计算P(A|B).极大地减小了计算复杂度.

回到商品推荐的场景,当顾客购买了多件商品再推荐顾客购买其他的多件商品(而不是单件商品) 似乎更自然一些.

apriori不能用来直接推荐多件商品. 但可以"链式"推荐:
假设
```python
    a,b->c       confidence 0.7        support 0.05 
    a,b,c->d     confidence 0.6	       support 0.04
    ...
```
当顾客的购物车里有商品a和商品b,这时候按最大似然(也就是置信度最大)给顾客推荐了商品c,顾客把商品c加入了购物车,再次遍历规则集合,继续按最大似然推荐d即可.

显然,经过"链式"推荐,每次推荐的商品的最大似然概率是慢慢减小的.
没必要直接推荐商品{c,d},因为:
$$P({c,d}|{a,b}) <= P({c}|{a,b}) and P({c,d}|{a,b})<= P({d}|{a,b})$$只需根据购物车的商品推测最有可能购买的一个商品即可.

至于这里的概率,是通过频率来近似的.

置信度的不足:

为什么需要提升度呢？
比如：100条购买记录中，有60条包含牛奶，75条包含面包，其中有40条两者都包含。关联规则（牛奶，面包）的支持度为0.4，看似很高，但其实这个关联规则是一个误导。在用户购买了牛奶的前提下，有(40/60 = ) 0.67的概率去购买面包，而在没有任何前提条件时，用户反而有(75/100 = ) 0.75的概率去购买面包。也就是说，设置了购买牛奶的前提会降低用户购买面包的概率，也就是说面包和牛奶是互斥的。
如果lift=1，说明两个事项没有任何关联；如果lift<1，说明A事件的发生与B事件是相斥的。一般在数据挖掘中当提升度大于3时，我们才承认挖掘出的关联规则是有价值的。

提升度（lift）

通俗解释：提升度反映了“物品集A的出现”对物品集B的出现概率发生了多大的变化。

概率描述：lift（A->B）=confidence（A->B）/support(B)=P(B|A)/P(B) = P(B,A)/(P(A)*P(B)) = P(A,B)/(P(A)*P(B))





https://en.wikipedia.org/wiki/Lift_(data_mining)



lift

算法优化:

>2000年Jiawei Han等人提出了基于FP树生成频繁项集的FP-growth算法。该算法只进行2次数据库扫描且它不使用侯选集，直接压缩数据库成一个频繁模式树，最后通过这棵树生成关联规则。研究表明它比Apriori算法大约快一个数量级。

>Improvised Apriori Algorithm using frequent pattern tree for real time applications in data mining
https://arxiv.org/abs/1411.6224

Reference:    
https://blog.csdn.net/dream_angel_z/article/details/46355803     
https://github.com/FanhuaandLuomu/Apriori_Learning/blob/master/Apriori_numpy.py        
https://spaces.ac.cn/archives/5525        
https://colab.research.google.com/drive/1aXmA3sR5GefpBDM2zdnImpL9X6yQ4hAh
https://en.wikipedia.org/wiki/Apriori_algorithm



结果可视化
apriori输出的规则集能否放入树或者森林,例如
首先将规则左边的集合 按字典序排序
然后将
a,b->c
a,b->e
放入以a为跟的树

将
b,a ->c
放入以b为跟的树
根据前缀在相应的树里添加规则

但是有一个问题,假设出现了规则a,c->b
那么树a里就存在回路了.

因此,如果想可视化话,还是得把规则集放入一个图中或多个图中.


有人这么做了:

https://cran.r-project.org/web/packages/arulesViz/vignettes/arulesViz.pdf
>Figure 9: Graph-based visualization with items and rules as vertices.




R语言中有专用于可视化 assiciation rules 的R包.
具体实现见:
https://github.com/kifish/R-notes/tree/master/plot_rules

[交互式demo](https://kifish.github.io/R-notes/plot_rules/qfs.html)


