---
layout: post
published : true
tags : [ml]
---


可以看这份材料
-http://www.cnblogs.com/pinard/p/6140514.html

gbdt回归：
原始的gbdt其实思想很简单，不是通过样本的bagging,而是通过多轮迭代。  
每次的目标是拟合残差或者说是让输出增量逼近上一轮的残差(error),从而加上这个增量后最小化error。
有点像决策树桩叠起来的，把样本分为多个不同的节点(node)，每个node输出是一样的，不同node输出不一样。

gbdt分类：
要解决离散的问题，主要有两种方法，
指数损失函数，此时退化为adaboost,另一种用基于逻辑回归的对数似然损失函数。

>
https://www.zhihu.com/question/41354392/answer/98658997
- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。
- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。
- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
- ...

>
xgboost号称scalable tree boosting system，其中非常重要的scalability有下面几个层面的含义，也算是与gbdt的核心差异所在：（按李航的机器学习=模型+策略+算法的思路，不一定完全对应～）   
1、模型的scalability，弱分类器除cart外也支持lr和linear
2、策略的scalability，可以支持不同的loss functions，来优化效果，只要一、二阶可导即可    
3、算法的scalability，做了很多细节工作，来优化参数学习和迭代速度，特征压缩技术，bagging学习中的特征抽样，特征选择与阈值分裂的分位方法和并行方法等     
4、数据的scalability，因为3中的优化，支持B级别的快速训练和建模；同时也因为加上了正则项和随机特征抽样，减少了过拟合问题


>
GBDT采用的是数值优化的思维, 用的最速下降法去求解Loss Function的最优解, 其中用CART决策树去拟合负梯度, 用牛顿法求步长.   
XGboost用的解析的思维, 对Loss Function展开到二阶近似, 求得解析解, 用解析解作为Gain来建立决策树, 使得Loss Function最优.
